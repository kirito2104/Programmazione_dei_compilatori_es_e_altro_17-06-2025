
''' 
Esempio di ComplessitÃ  Quadratica (O(nÂ²)) :
'''

# La complessitÃ  quadratica Ã¨ indicata come O(nÂ²) e significa che il tempo di esecuzione cresce
# proporzionalmente al quadrato della dimensione dell'input.
# Ecco alcuni esempi di algoritmi con complessitÃ  O(nÂ²), escludendo il Merge Sort (che ha O(n log n)).

''' 
1ï¸âƒ£ Bubble Sort 
'''

def bubble_sort(arr):
    # Algoritmo di ordinamento inefficiente che confronta coppie adiacenti
    n = len(arr)
    for i in range(n):  # Ciclo esterno
        for j in range(0, n - i - 1):  # Ciclo interno
            if arr[j] > arr[j + 1]:  # Scambio se necessario
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
    return arr

# Esempio di utilizzo
print(bubble_sort([5, 3, 8, 4, 2]))

''' 
ğŸ“Œ ComplessitÃ : 
'''
# - Due cicli annidati â†’ O(nÂ²)
# - Peggiore e medio caso: O(nÂ²)
# - Miglior caso: O(n) (se l'array Ã¨ giÃ  ordinato)

''' 
2 Selection Sort 
'''

def selection_sort(arr):
    # Algoritmo che seleziona ripetutamente il valore piÃ¹ piccolo
    n = len(arr)
    for i in range(n):
        min_idx = i
        for j in range(i + 1, n):  # Cerca il minimo nel resto dell'array
            if arr[j] < arr[min_idx]:
                min_idx = j
        arr[i], arr[min_idx] = arr[min_idx], arr[i]  # Scambia gli elementi
    return arr

# Esempio di utilizzo
print(selection_sort([64, 25, 12, 22, 11]))

''' 
ğŸ“Œ ComplessitÃ : 
'''
# - Due cicli annidati â†’ O(nÂ²)
# - Peggiore, medio e miglior caso: O(nÂ²)

''' 
3ï¸âƒ£ Insertion Sort 
'''

def insertion_sort(arr):
    # Algoritmo che inserisce ogni elemento nella posizione corretta
    n = len(arr)
    for i in range(1, n):
        key = arr[i]
        j = i - 1
        while j >= 0 and arr[j] > key:  # Sposta gli elementi per trovare il posto corretto
            arr[j + 1] = arr[j]
            j -= 1
        arr[j + 1] = key
    return arr

# Esempio di utilizzo
print(insertion_sort([9, 5, 1, 4, 3]))

''' 
ğŸ“Œ ComplessitÃ : 
'''
# - Peggiore e medio caso: O(nÂ²)
# - Miglior caso: O(n) (se l'array Ã¨ giÃ  ordinato)

''' 
4ï¸âƒ£ Algoritmo per trovare tutte le coppie in un array 
'''

def find_pairs(arr):
    # Confronta tutte le coppie possibili in un array
    n = len(arr)
    for i in range(n):
        for j in range(i + 1, n):  # Confronta ogni coppia
            print(f"Coppia: ({arr[i]}, {arr[j]})")

# Esempio di utilizzo
find_pairs([1, 2, 3, 4])

''' 
ğŸ“Œ ComplessitÃ : 
'''
# - Due cicli annidati â†’ O(nÂ²)

''' 
5ï¸âƒ£ Moltiplicazione di matrici 
'''

def multiply_matrices(A, B):
    # Moltiplicazione tra due matrici n x n
    n = len(A)
    result = [[0] * n for _ in range(n)]
    for i in range(n):  # Scorre le righe
        for j in range(n):  # Scorre le colonne
            for k in range(n):  # Calcola il prodotto scalare (O(nÂ³) in realtÃ )
                result[i][j] += A[i][k] * B[k][j]
    return result

# Esempio di utilizzo
A = [[1, 2], [3, 4]]
B = [[5, 6], [7, 8]]
print(multiply_matrices(A, B))

''' 
ğŸ“Œ ComplessitÃ : 
'''
# - In genere O(nÂ³), ma versioni ottimizzate riducono a O(nÂ²) in alcuni casi.

''' 
6ï¸âƒ£ Problema del Viaggiatore (Approccio Greedy) 
'''

import random

def nearest_neighbor_tsp(distances):
    # Soluzione approssimata al problema del Commesso Viaggiatore (TSP)
    n = len(distances)
    visited = [False] * n
    path = [0]  # Partiamo dal nodo 0
    visited[0] = True
    for _ in range(n - 1):  # Visitiamo tutti i nodi una volta
        last = path[-1]
        next_city = min(
            [(i, distances[last][i]) for i in range(n) if not visited[i]], 
            key=lambda x: x[1]
        )[0]
        path.append(next_city)
        visited[next_city] = True
    return path

# Esempio di utilizzo
distances = [[0, 2, 9, 10],
             [1, 0, 6, 4],
             [15, 7, 0, 8],
             [6, 3, 12, 0]]
print(nearest_neighbor_tsp(distances))

''' 
ğŸ“Œ ComplessitÃ : 
'''
# - Il ciclo principale ha O(n) iterazioni e per ogni iterazione cerchiamo il nodo piÃ¹ vicino O(n) â†’ O(nÂ²).

''' 
ğŸ“Œ Conclusione 
'''
# La complessitÃ  O(nÂ²) appare quando:
# - Usi due cicli annidati che dipendono dalla dimensione dell'input.
# - Devi confrontare tutti gli elementi con tutti gli altri (es. selezione di coppie).
# - Hai algoritmi di ordinamento semplici come Bubble Sort, Selection Sort e Insertion Sort.
# - Fai operazioni su strutture bidimensionali come matrici.
#
# Per ridurre la complessitÃ  quadratica, puoi spesso trovare algoritmi piÃ¹ efficienti,
# come QuickSort (O(n log n)) o Algoritmi Greedy e Programmazione Dinamica.


#############################################################################################################
''' 
Mentre la complessitÃ  quadratica: 
'''

''' 
ğŸ“Œ ComplessitÃ  Quadratica (O(nÂ²)) 
'''

# La complessitÃ  quadratica Ã¨ indicata come O(nÂ²) e significa che il tempo di esecuzione cresce
# proporzionalmente al quadrato della dimensione dell'input.
# Ecco alcuni esempi di algoritmi con complessitÃ  O(nÂ²), 
# escludendo il Merge Sort (che ha O(n log n)).

''' 
1ï¸âƒ£ Bubble Sort 
'''

def bubble_sort(arr):
    # Algoritmo di ordinamento inefficiente che confronta coppie adiacenti
    n = len(arr)
    for i in range(n):  # Ciclo esterno
        for j in range(0, n - i - 1):  # Ciclo interno
            if arr[j] > arr[j + 1]:  # Scambio se necessario
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
    return arr

# Esempio di utilizzo
print(bubble_sort([5, 3, 8, 4, 2]))

''' 
ğŸ“Œ ComplessitÃ : 
'''
# - Due cicli annidati â†’ O(nÂ²)
# - Peggiore e medio caso: O(nÂ²)
# - Miglior caso: O(n) (se l'array Ã¨ giÃ  ordinato)

''' 
ğŸ“Œ Differenza tra ComplessitÃ  Quadratica (O(nÂ²)) e Logaritmica (O(log n)) 
'''
# La differenza principale tra O(nÂ²) e O(log n) Ã¨ il modo in cui il tempo di esecuzione cresce
# con l'aumento dell'input.
#
# - **O(nÂ²)** significa che il tempo di esecuzione aumenta esponenzialmente rispetto all'input.
#   Se l'input diventa 10 volte piÃ¹ grande, il tempo di esecuzione diventa 100 volte piÃ¹ grande.
#
# - **O(log n)** significa che il tempo di esecuzione cresce molto piÃ¹ lentamente.
#   Se l'input raddoppia, il numero di operazioni aggiuntive Ã¨ solo un piccolo incremento.
#
# **PerchÃ© O(log n) Ã¨ piÃ¹ efficiente?**
# - Algoritmi con complessitÃ  **logaritmica** riducono il problema in sottoproblemi sempre piÃ¹ piccoli.
# - L'esempio piÃ¹ comune Ã¨ la **Ricerca Binaria**, che dimezza l'input ad ogni passo, risultando molto veloce.
# - Un altro esempio Ã¨ **QuickSort**, che ha complessitÃ  O(n log n) nel caso medio.
#
# **Esempio di Ricerca Binaria (O(log n))**:
#
# def binary_search(arr, target):
#     left, right = 0, len(arr) - 1
#     while left <= right:
#         mid = (left + right) // 2
#         if arr[mid] == target:
#             return mid
#         elif arr[mid] < target:
#             left = mid + 1
#         else:
#             right = mid - 1
#     return -1
#
# **Conclusione:**
# - Se possibile, Ã¨ sempre meglio usare algoritmi **logaritmici** o **O(n log n)** invece di O(nÂ²).
# - Per grandi dataset, un algoritmo logaritmico sarÃ  **migliaia di volte piÃ¹ veloce** rispetto a uno quadratico.

''' 
ğŸš€ Ora hai una panoramica sulla differenza tra O(nÂ²) e O(log n)! 
'''



